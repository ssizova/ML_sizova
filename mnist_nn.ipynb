{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.layers import Dense, Conv2D, Input, Flatten, MaxPool2D, Dropout\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x72 with 15 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (60000, 28, 28) (60000, 1)\n",
      "Test: (10000, 28, 28) (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "plt.figure(figsize=(15, 1))\n",
    "for i in range(15):\n",
    "    plt.subplot(1, 15, i+1)\n",
    "    plt.imshow(x_train[i])\n",
    "    plt.title(y_train[i])\n",
    "plt.show()\n",
    "y_train = np.expand_dims(y_train, -1)\n",
    "y_test = np.expand_dims(y_test, -1)\n",
    "print('Train:', x_train.shape, y_train.shape)\n",
    "print('Test:', x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(inp):\n",
    "    layer = Conv2D(filters=8, kernel_size=(3, 3), padding='same')(inp)\n",
    "    layer = MaxPool2D(pool_size=(2, 2))(layer)\n",
    "    layer = Conv2D(filters=16, kernel_size=(3, 3), padding='same')(layer)\n",
    "    layer = Flatten()(layer)\n",
    "    layer = Dense(units=128)(layer)\n",
    "    layer = Dropout(0.2)(layer)\n",
    "    out = Dense(units=10, activation='softmax')(layer)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "Model: \"functional_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 28, 28, 8)         80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 14, 14, 16)        1168      \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 128)               401536    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 404,074\n",
      "Trainable params: 404,074\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "   2/2143 [..............................] - ETA: 9:38 - loss: 2.2310 - accuracy: 0.2679WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0080s vs `on_train_batch_end` time: 0.5338s). Check your callbacks.\n",
      "2143/2143 [==============================] - 17s 8ms/step - loss: 0.2095 - accuracy: 0.9372 - val_loss: 6.7588 - val_accuracy: 0.1135\n",
      "Epoch 2/10\n",
      "2143/2143 [==============================] - 20s 9ms/step - loss: 0.1093 - accuracy: 0.9669 - val_loss: 10.3716 - val_accuracy: 0.1135\n",
      "Epoch 3/10\n",
      "2143/2143 [==============================] - 19s 9ms/step - loss: 0.0953 - accuracy: 0.9705 - val_loss: 10.0908 - val_accuracy: 0.1135\n",
      "Epoch 4/10\n",
      "2143/2143 [==============================] - 19s 9ms/step - loss: 0.0862 - accuracy: 0.9739 - val_loss: 12.5222 - val_accuracy: 0.1135\n",
      "Epoch 5/10\n",
      "2143/2143 [==============================] - 19s 9ms/step - loss: 0.0792 - accuracy: 0.9755 - val_loss: 11.0860 - val_accuracy: 0.1135\n",
      "Epoch 6/10\n",
      "2143/2143 [==============================] - 19s 9ms/step - loss: 0.0733 - accuracy: 0.9767 - val_loss: 11.3381 - val_accuracy: 0.1135\n",
      "Epoch 7/10\n",
      "2143/2143 [==============================] - 19s 9ms/step - loss: 0.0717 - accuracy: 0.9767 - val_loss: 14.2929 - val_accuracy: 0.1135\n",
      "Epoch 8/10\n",
      "2143/2143 [==============================] - 19s 9ms/step - loss: 0.0690 - accuracy: 0.9777 - val_loss: 12.9938 - val_accuracy: 0.1135\n",
      "Epoch 9/10\n",
      "2143/2143 [==============================] - 19s 9ms/step - loss: 0.0643 - accuracy: 0.9797 - val_loss: 15.0239 - val_accuracy: 0.1135\n",
      "Epoch 10/10\n",
      "2143/2143 [==============================] - 19s 9ms/step - loss: 0.0630 - accuracy: 0.9802 - val_loss: 14.2172 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14f53431488>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "\n",
    "x_train = x_train.astype(np.float32) / 255\n",
    "x_test = x_test.astype(np.float32) / 255\n",
    "\n",
    "y_train_cl = to_categorical(y_train, num_classes=10)\n",
    "y_test_cl = to_categorical(y_test, num_classes=10)\n",
    "print(y_train_cl.shape)\n",
    "\n",
    "inp = Input(shape=(28, 28, 1))\n",
    "\n",
    "out = create_model(inp)\n",
    "model = Model(inp, out)\n",
    "model.summary()\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "log_dir = \"logs/mnist_tb/\" + 'adam'\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model.fit(x=x_train, \n",
    "          y=y_train_cl, \n",
    "          batch_size=28,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test, y_test_cl), \n",
    "          callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 28, 28, 8)         80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 16)        1168      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               401536    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 404,074\n",
      "Trainable params: 404,074\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "   2/2143 [..............................] - ETA: 1:18 - loss: 0.0885 - accuracy: 0.9643WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0080s vs `on_train_batch_end` time: 0.0648s). Check your callbacks.\n",
      "2143/2143 [==============================] - 23s 11ms/step - loss: 0.0554 - accuracy: 0.9824 - val_loss: 0.0825 - val_accuracy: 0.9778\n",
      "Epoch 2/10\n",
      "2143/2143 [==============================] - 23s 11ms/step - loss: 0.0555 - accuracy: 0.9830 - val_loss: 0.1200 - val_accuracy: 0.9699\n",
      "Epoch 3/10\n",
      "2143/2143 [==============================] - 22s 10ms/step - loss: 0.0543 - accuracy: 0.9834 - val_loss: 0.0995 - val_accuracy: 0.9756\n",
      "Epoch 4/10\n",
      "2143/2143 [==============================] - 23s 11ms/step - loss: 0.0530 - accuracy: 0.9840 - val_loss: 0.1330 - val_accuracy: 0.9664\n",
      "Epoch 5/10\n",
      "2143/2143 [==============================] - 23s 11ms/step - loss: 0.0514 - accuracy: 0.9837 - val_loss: 0.1092 - val_accuracy: 0.9740\n",
      "Epoch 6/10\n",
      "2143/2143 [==============================] - 22s 10ms/step - loss: 0.0525 - accuracy: 0.9838 - val_loss: 0.1020 - val_accuracy: 0.9754\n",
      "Epoch 7/10\n",
      "2143/2143 [==============================] - 22s 10ms/step - loss: 0.0509 - accuracy: 0.9845 - val_loss: 0.1002 - val_accuracy: 0.9764\n",
      "Epoch 8/10\n",
      "2143/2143 [==============================] - 22s 10ms/step - loss: 0.0503 - accuracy: 0.9843 - val_loss: 0.1208 - val_accuracy: 0.9712\n",
      "Epoch 9/10\n",
      "2143/2143 [==============================] - 23s 11ms/step - loss: 0.0493 - accuracy: 0.9848 - val_loss: 0.1368 - val_accuracy: 0.9701\n",
      "Epoch 10/10\n",
      "2143/2143 [==============================] - 22s 10ms/step - loss: 0.0496 - accuracy: 0.9852 - val_loss: 0.1349 - val_accuracy: 0.9727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a0f6dfa548>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = Model(inp, out)\n",
    "model2.summary()\n",
    "model2.compile(optimizer='RMSProp',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "log_dir = \"logs/mnist_tb/\" + 'RMSProp'\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model2.fit(x=x_train, \n",
    "          y=y_train_cl, \n",
    "          batch_size=28,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test, y_test_cl), \n",
    "          callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 28, 28, 8)         80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 16)        1168      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               401536    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 404,074\n",
      "Trainable params: 404,074\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "   2/2143 [..............................] - ETA: 1:09 - loss: 0.0617 - accuracy: 0.9821WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0080s vs `on_train_batch_end` time: 0.0549s). Check your callbacks.\n",
      "2143/2143 [==============================] - 21s 10ms/step - loss: 0.0273 - accuracy: 0.9913 - val_loss: 0.1046 - val_accuracy: 0.9768\n",
      "Epoch 2/10\n",
      "2143/2143 [==============================] - 20s 9ms/step - loss: 0.0229 - accuracy: 0.9927 - val_loss: 0.1033 - val_accuracy: 0.9771\n",
      "Epoch 3/10\n",
      "2143/2143 [==============================] - 20s 9ms/step - loss: 0.0225 - accuracy: 0.9930 - val_loss: 0.1020 - val_accuracy: 0.9771\n",
      "Epoch 4/10\n",
      "2143/2143 [==============================] - 20s 10ms/step - loss: 0.0233 - accuracy: 0.9925 - val_loss: 0.1015 - val_accuracy: 0.9773\n",
      "Epoch 5/10\n",
      "2143/2143 [==============================] - 20s 9ms/step - loss: 0.0219 - accuracy: 0.9932 - val_loss: 0.1005 - val_accuracy: 0.9766\n",
      "Epoch 6/10\n",
      "2143/2143 [==============================] - 20s 9ms/step - loss: 0.0209 - accuracy: 0.9934 - val_loss: 0.1010 - val_accuracy: 0.9766\n",
      "Epoch 7/10\n",
      "2143/2143 [==============================] - 21s 10ms/step - loss: 0.0212 - accuracy: 0.9933 - val_loss: 0.1003 - val_accuracy: 0.9767\n",
      "Epoch 8/10\n",
      "2143/2143 [==============================] - 20s 10ms/step - loss: 0.0220 - accuracy: 0.9933 - val_loss: 0.0997 - val_accuracy: 0.9769\n",
      "Epoch 9/10\n",
      "2143/2143 [==============================] - 20s 9ms/step - loss: 0.0210 - accuracy: 0.9935 - val_loss: 0.0995 - val_accuracy: 0.9770\n",
      "Epoch 10/10\n",
      "2143/2143 [==============================] - 20s 9ms/step - loss: 0.0203 - accuracy: 0.9937 - val_loss: 0.0996 - val_accuracy: 0.9769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a0f6d25948>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = Model(inp, out)\n",
    "model3.summary()\n",
    "model3.compile(optimizer='Adagrad',\n",
    "               loss='categorical_crossentropy',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "log_dir = \"logs/mnist_tb/\" + 'Adagrad'\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model3.fit(x=x_train, \n",
    "          y=y_train_cl, \n",
    "          batch_size=28,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test, y_test_cl), \n",
    "           callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 28, 28, 8)         80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 16)        1168      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               401536    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 404,074\n",
      "Trainable params: 404,074\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "   2/2143 [..............................] - ETA: 4:08 - loss: 0.0039 - accuracy: 1.0000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0100s vs `on_train_batch_end` time: 0.2227s). Check your callbacks.\n",
      "2143/2143 [==============================] - 28s 13ms/step - loss: 0.0239 - accuracy: 0.9926 - val_loss: 0.1019 - val_accuracy: 0.9760\n",
      "Epoch 2/10\n",
      "2143/2143 [==============================] - 29s 14ms/step - loss: 0.0206 - accuracy: 0.9934 - val_loss: 0.1030 - val_accuracy: 0.9762\n",
      "Epoch 3/10\n",
      "2143/2143 [==============================] - 28s 13ms/step - loss: 0.0192 - accuracy: 0.9941 - val_loss: 0.1000 - val_accuracy: 0.9768\n",
      "Epoch 4/10\n",
      "2143/2143 [==============================] - 29s 14ms/step - loss: 0.0187 - accuracy: 0.9937 - val_loss: 0.1006 - val_accuracy: 0.9764\n",
      "Epoch 5/10\n",
      "2143/2143 [==============================] - 31s 14ms/step - loss: 0.0174 - accuracy: 0.9945 - val_loss: 0.1043 - val_accuracy: 0.9765\n",
      "Epoch 6/10\n",
      "2143/2143 [==============================] - 30s 14ms/step - loss: 0.0167 - accuracy: 0.9943 - val_loss: 0.1007 - val_accuracy: 0.9765\n",
      "Epoch 7/10\n",
      "2143/2143 [==============================] - 30s 14ms/step - loss: 0.0154 - accuracy: 0.9954 - val_loss: 0.1074 - val_accuracy: 0.9759\n",
      "Epoch 8/10\n",
      "2143/2143 [==============================] - 29s 14ms/step - loss: 0.0154 - accuracy: 0.9954 - val_loss: 0.1042 - val_accuracy: 0.9766\n",
      "Epoch 9/10\n",
      "2143/2143 [==============================] - 30s 14ms/step - loss: 0.0145 - accuracy: 0.9956 - val_loss: 0.1093 - val_accuracy: 0.9757\n",
      "Epoch 10/10\n",
      "2143/2143 [==============================] - 29s 14ms/step - loss: 0.0140 - accuracy: 0.9954 - val_loss: 0.1116 - val_accuracy: 0.9748\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a0f6a29388>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4 = Model(inp, out)\n",
    "model4.summary()\n",
    "model4.compile(optimizer='Adamax',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "log_dir = \"logs/mnist_tb/\" + 'Adamax'\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model4.fit(x=x_train, \n",
    "          y=y_train_cl, \n",
    "          batch_size = 28,\n",
    "          epochs=10, \n",
    "          validation_data=(x_test, y_test_cl), \n",
    "          callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph of accuracy for validation set for different optimizers shows that number of epochs is sufficient: the fucntion reaches its asymptote. The accuracy values for different optimizers are close to each other, but \"AdamMax\" (4) and  \"Adamgrad\" (3) show better performance than Adam (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](accuracy_models.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss for validation function is the lowest for Adam optimizer. So, as the accuracy does not differ much for all the optimizers, we can say that Adam optimizer is preferable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](tb_loss_acc.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_overfitting(inp):\n",
    "    layer = Conv2D(filters=24, kernel_size=(3, 3), padding='same')(inp)\n",
    "    layer = MaxPool2D(pool_size=(2, 2))(layer)\n",
    "    #layer = MaxPool2D(pool_size=(2, 2))(layer)\n",
    "\n",
    "    layer = Flatten()(layer)\n",
    "    layer = Dense(units=64)(layer)\n",
    "    #layer = Dropout(0.2)(layer)\n",
    "    out = Dense(units=10, activation='softmax')(layer)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 28, 28, 24)        240       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 24)        0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4704)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                301120    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 302,010\n",
      "Trainable params: 302,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "    2/12000 [..............................] - ETA: 15:05 - loss: 0.0012 - accuracy: 1.0000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0050s vs `on_train_batch_end` time: 0.1470s). Check your callbacks.\n",
      "12000/12000 [==============================] - 58s 5ms/step - loss: 0.0653 - accuracy: 0.9794 - val_loss: 0.0993 - val_accuracy: 0.9706\n",
      "Epoch 2/15\n",
      "12000/12000 [==============================] - 58s 5ms/step - loss: 0.0639 - accuracy: 0.9796 - val_loss: 0.0899 - val_accuracy: 0.9740\n",
      "Epoch 3/15\n",
      "12000/12000 [==============================] - 60s 5ms/step - loss: 0.0623 - accuracy: 0.9806 - val_loss: 0.0953 - val_accuracy: 0.9719\n",
      "Epoch 4/15\n",
      "12000/12000 [==============================] - 60s 5ms/step - loss: 0.0599 - accuracy: 0.9812 - val_loss: 0.0718 - val_accuracy: 0.9801\n",
      "Epoch 5/15\n",
      "12000/12000 [==============================] - 58s 5ms/step - loss: 0.0579 - accuracy: 0.9813 - val_loss: 0.0842 - val_accuracy: 0.9758\n",
      "Epoch 6/15\n",
      "12000/12000 [==============================] - 58s 5ms/step - loss: 0.0574 - accuracy: 0.9817 - val_loss: 0.0835 - val_accuracy: 0.9762\n",
      "Epoch 7/15\n",
      "12000/12000 [==============================] - 60s 5ms/step - loss: 0.0574 - accuracy: 0.9817 - val_loss: 0.0868 - val_accuracy: 0.9755\n",
      "Epoch 8/15\n",
      "12000/12000 [==============================] - 59s 5ms/step - loss: 0.0575 - accuracy: 0.9814 - val_loss: 0.1240 - val_accuracy: 0.9682\n",
      "Epoch 9/15\n",
      "12000/12000 [==============================] - 61s 5ms/step - loss: 0.0557 - accuracy: 0.9823 - val_loss: 0.0844 - val_accuracy: 0.9766\n",
      "Epoch 10/15\n",
      "12000/12000 [==============================] - 59s 5ms/step - loss: 0.0545 - accuracy: 0.9827 - val_loss: 0.0911 - val_accuracy: 0.9747\n",
      "Epoch 11/15\n",
      "12000/12000 [==============================] - 58s 5ms/step - loss: 0.0550 - accuracy: 0.9827 - val_loss: 0.1112 - val_accuracy: 0.9692\n",
      "Epoch 12/15\n",
      "12000/12000 [==============================] - 59s 5ms/step - loss: 0.0517 - accuracy: 0.9829 - val_loss: 0.0965 - val_accuracy: 0.9747\n",
      "Epoch 13/15\n",
      "12000/12000 [==============================] - 58s 5ms/step - loss: 0.0518 - accuracy: 0.9830 - val_loss: 0.0980 - val_accuracy: 0.9754\n",
      "Epoch 14/15\n",
      "12000/12000 [==============================] - 59s 5ms/step - loss: 0.0523 - accuracy: 0.9836 - val_loss: 0.1267 - val_accuracy: 0.9656\n",
      "Epoch 15/15\n",
      "12000/12000 [==============================] - 60s 5ms/step - loss: 0.0495 - accuracy: 0.9839 - val_loss: 0.0871 - val_accuracy: 0.9769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14f542920c8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_overfitting = create_model_overfitting(inp)\n",
    "model_overfitting = Model(inp, out_overfitting)\n",
    "model_overfitting.summary()\n",
    "model_overfitting.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "log_dir = \"logs/mnist_overr/\" + 'adam'\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model.fit(x=x_train, \n",
    "          y=y_train_cl, \n",
    "          epochs=15, \n",
    "          batch_size= 5,\n",
    "          validation_data=(x_test, y_test_cl), \n",
    "          callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting: The plot of training loss continues to decrease.\n",
    "The plot of validation loss decreases to a point and then begins increasing again.\n",
    "The inflection point in validation loss may be the point at which training could be halted as experience after that point shows the dynamics of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](overfit.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation(image,angle):\n",
    "    #image = ndimage.rotate(image, angle) \n",
    "    image = image.reshape((28,28))\n",
    "    return image\n",
    "    \n",
    "\n",
    "def blur(image):\n",
    "    kernel = (random.randint(1,10), random.randint(1,10))\n",
    "    #image = cv2.blur(image, kernel)\n",
    "    image = image.reshape((28,28))\n",
    "    return image\n",
    "\n",
    "def flip(image):\n",
    "    flip = random.randint(-1,1)\n",
    "    #image = cv2.flip(image, flip)\n",
    "    image = image.reshape((28,28))\n",
    "    return image\n",
    "    \n",
    "def augmentations(image, type):\n",
    "    cases = {\n",
    "        '1': lambda image: rotation(image, random.randrange(0,360)),\n",
    "        '2': lambda image: blur(image),\n",
    "        '3': lambda image: flip(image)\n",
    "    }\n",
    "    result = cases[str(type)]\n",
    "    return result(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train_augmented = x_train\n",
    "print(np.shape(x_train[0]))\n",
    "i = 0\n",
    "for image in x_train:\n",
    "    x_train_augmented[i] = augmentations(image, random.randint(1,3))\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 28, 28, 8)         80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 14, 14, 16)        1168      \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               401536    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 404,074\n",
      "Trainable params: 404,074\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "   2/2143 [..............................] - ETA: 9:44 - loss: 2.3001 - accuracy: 0.1964WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0080s vs `on_train_batch_end` time: 0.5383s). Check your callbacks.\n",
      "2143/2143 [==============================] - 16s 7ms/step - loss: 0.4771 - accuracy: 0.8557 - val_loss: 0.2987 - val_accuracy: 0.9135\n",
      "Epoch 2/15\n",
      "2143/2143 [==============================] - 21s 10ms/step - loss: 0.3236 - accuracy: 0.9064 - val_loss: 0.2847 - val_accuracy: 0.9195\n",
      "Epoch 3/15\n",
      "2143/2143 [==============================] - 19s 9ms/step - loss: 0.3089 - accuracy: 0.9111 - val_loss: 0.2696 - val_accuracy: 0.9214\n",
      "Epoch 4/15\n",
      "2143/2143 [==============================] - 19s 9ms/step - loss: 0.2955 - accuracy: 0.9150 - val_loss: 0.2619 - val_accuracy: 0.9257\n",
      "Epoch 5/15\n",
      "2143/2143 [==============================] - 19s 9ms/step - loss: 0.2758 - accuracy: 0.9213 - val_loss: 0.2515 - val_accuracy: 0.9276\n",
      "Epoch 6/15\n",
      "2143/2143 [==============================] - 19s 9ms/step - loss: 0.2539 - accuracy: 0.9273 - val_loss: 0.2192 - val_accuracy: 0.9389\n",
      "Epoch 7/15\n",
      "2143/2143 [==============================] - 19s 9ms/step - loss: 0.2338 - accuracy: 0.9322 - val_loss: 0.2204 - val_accuracy: 0.9378\n",
      "Epoch 8/15\n",
      "2143/2143 [==============================] - 20s 10ms/step - loss: 0.2176 - accuracy: 0.9377 - val_loss: 0.1882 - val_accuracy: 0.9476\n",
      "Epoch 9/15\n",
      "2143/2143 [==============================] - 21s 10ms/step - loss: 0.1966 - accuracy: 0.9429 - val_loss: 0.1669 - val_accuracy: 0.9541\n",
      "Epoch 10/15\n",
      "2143/2143 [==============================] - 19s 9ms/step - loss: 0.1717 - accuracy: 0.9506 - val_loss: 0.1508 - val_accuracy: 0.9574\n",
      "Epoch 11/15\n",
      "2143/2143 [==============================] - 19s 9ms/step - loss: 0.1528 - accuracy: 0.9560 - val_loss: 0.1282 - val_accuracy: 0.9625\n",
      "Epoch 12/15\n",
      "2143/2143 [==============================] - 19s 9ms/step - loss: 0.1365 - accuracy: 0.9608 - val_loss: 0.1189 - val_accuracy: 0.9648\n",
      "Epoch 13/15\n",
      "2143/2143 [==============================] - 19s 9ms/step - loss: 0.1245 - accuracy: 0.9631 - val_loss: 0.1070 - val_accuracy: 0.9679\n",
      "Epoch 14/15\n",
      "2143/2143 [==============================] - 19s 9ms/step - loss: 0.1146 - accuracy: 0.9661 - val_loss: 0.1022 - val_accuracy: 0.9690\n",
      "Epoch 15/15\n",
      "2143/2143 [==============================] - 19s 9ms/step - loss: 0.1091 - accuracy: 0.9679 - val_loss: 0.0994 - val_accuracy: 0.9714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14f54135948>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_augmented, x_test = x_train_augmented / 255.0, x_test / 255.0\n",
    "out5 = create_model(inp)\n",
    "model5 = Model(inp, out5)\n",
    "model5.summary()\n",
    "model5.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "log_dir = \"logs/mnist/\" + 'augmentation'\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model5.fit(x=x_train_augmented, \n",
    "          y=y_train_cl, \n",
    "          epochs=15, \n",
    "          batch_size = 28,\n",
    "          validation_data=(x_test, y_test_cl), \n",
    "          callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](augmentation.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
